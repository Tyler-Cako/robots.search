{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e418410",
   "metadata": {},
   "source": [
    "Let's compare our search engine to simple google results. We will use the same query, with the exception that our google search will have the added keywords \"google deepmind paper\" to get more precise results. We will then compare the similarity scores between the top 10 abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53556117",
   "metadata": {},
   "source": [
    "eg.\n",
    "\n",
    "query: \"computer vision\"\n",
    "\n",
    "-> google: \"computer vision google deepmind paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DocumentSearch, Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f696cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://deepmind.google/research/publications/83299',\n",
       "  'title': 'Are vision-language models shape or texture biased and can we steer them?',\n",
       "  'authors': ['Paul Gavrikov',\n",
       "   'Jovita Lukasik',\n",
       "   'Steffen Jung',\n",
       "   'Robert Geirhos',\n",
       "   'Muhammad Jehanzeb Mirza',\n",
       "   'Margret Keuper',\n",
       "   'Janis Keuper'],\n",
       "  'tags': ['video', 'vision', 'models', 'image', 'visual'],\n",
       "  'abstract': 'Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. As a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. As expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. While this direct impact of language-informed training on a model’s visual perception is intriguing, it raises further questions on our ability to actively steer a model’s output so that its prediction is based on particular visual cues of the user’s choice. Interestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful.',\n",
       "  'similarity': 0.4286},\n",
       " {'url': 'https://deepmind.google/research/publications/108549',\n",
       "  'title': 'Veo 2 15 April 2025                   Mixture of Nested Experts: Adaptive Processing of Visual Tokens',\n",
       "  'authors': ['Gagan Jain',\n",
       "   'Nidhi Hegde',\n",
       "   'Aditya Kusupati',\n",
       "   'Arsha Nagrani',\n",
       "   'Shyamal Buch',\n",
       "   'Prateek Jain',\n",
       "   'Anurag Arnab',\n",
       "   'Sujoy Paul'],\n",
       "  'tags': ['video', 'experts', 'data', 'models', 'learning'],\n",
       "  'abstract': 'The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over two-fold. We validate our approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. We further highlight MoNE’s adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.',\n",
       "  'similarity': 0.4286},\n",
       " {'url': 'https://deepmind.google/research/publications/26336',\n",
       "  'title': 'TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement',\n",
       "  'authors': ['Carl Doersch',\n",
       "   'Yi Yang',\n",
       "   'Mel Vecerik',\n",
       "   'Dilara Gokay',\n",
       "   'Yusuf Aytar',\n",
       "   'Joao Carreira',\n",
       "   'Andrew Zisserman',\n",
       "   'Ankush Gupta'],\n",
       "  'tags': ['video', 'data', 'tasks', 'models', 'image'],\n",
       "  'abstract': 'We present a novel model for Tracking Any Point (TAP) that effectively tracks any queried point on any physical surface throughout a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the TAP-Vid benchmark, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model facilitates fast inference on long and high-resolution video sequences. On a modern GPU, our implementation has the capacity to track points faster than real-time, and can be flexibly extended to higher-resolution videos. Given the high-quality trajectories extracted from a large dataset, we demonstrate a proof-of-concept diffusion model which generates trajectories from static images, enabling plausible animations. Visualizations, source code, and pretrained models can be found at https://deepmind-tapir.github.io/',\n",
       "  'similarity': 0.4286},\n",
       " {'url': 'https://deepmind.google/research/publications/50070',\n",
       "  'title': 'Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual Modalities',\n",
       "  'authors': ['AJ Piergiovanni',\n",
       "   'Isaac Noble',\n",
       "   'Dahun Kim',\n",
       "   'Michael S. Ryoo',\n",
       "   'Victor Gomes',\n",
       "   'Anelia Angelova'],\n",
       "  'tags': ['video', 'audio', 'vision', 'scale', 'visual'],\n",
       "  'abstract': 'One of the main challenges of multimodal models is that they need to combine heterogeneous modalities (e.g. video, audio, text), which have different characteristics. For example, video and audio are obtained at much higher rates than text and are roughly aligned in time. They are not necessarily synchronized with text which is often present as a global context, e.g. a title or description. Furthermore, video and audio inputs are of much larger volumes, and can grow with the increase of the video lengths, which naturally requires more compute dedicated to these modalities and makes modeling of long-range dependencies harder. We here decouple the multimodal modeling, dividing it into separate, focused autoregressive models, processing the inputs according to the characteristics of the modalities and their sampling rates. More specifically, we propose a multimodal model, consisting of an autoregressive component for the time-synchronized modalities (audio and video), and an autoregressive component for modalities which are not necessarily aligned in time but are still sequential. To address the long-sequences of the video-audio inputs, we propose to further partition the video and audio sequences and autoregressively process their compact learned representations. To that end, we propose a Combiner mechanism, which models the audio-video information within a video snippet. The Combiner first learns to extract audio and visual features from raw spatio-temporal signals, and then learns to fuse these features producing compact but expressive representations per snippet, which are added to the subsequent autoregressive modeling in time. Our approach achieves the state-of-the-art on well established multimodal benchmarks, outperforming much larger models. It effectively addresses the high computational demand of media inputs by both learning compact representations, controlling the sequence length of the audio-video feature representations, and producing powerful visual and audio representations which allow for long-term dependencies and modeling of long-form video or video/audio inputs in time.',\n",
       "  'similarity': 0.4286},\n",
       " {'url': 'https://deepmind.google/research/publications/59160',\n",
       "  'title': 'Gemini and Whisk with Veo 2 15 April 2025                   Learning from One Continuous Video Stream',\n",
       "  'authors': ['Joao Carreira',\n",
       "   'Michael King',\n",
       "   'Viorica Patraucean',\n",
       "   'Dilara Gokay',\n",
       "   'Catalin Ionescu',\n",
       "   'Yi Yang',\n",
       "   'Daniel Zoran',\n",
       "   'Joseph Heyward',\n",
       "   'Carl Doersch',\n",
       "   'Yusuf Aytar',\n",
       "   'Dima Damen',\n",
       "   'Andrew Zisserman'],\n",
       "  'tags': ['video', 'data', 'learning', 'models', 'tasks'],\n",
       "  'abstract': 'We introduce a framework for online learning from a single continuous video stream -- the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little prior work on it. Our framework allows us to do a first deep dive into the topic and includes a collection of streams and tasks composed from two existing video datasets, plus methodology for performance evaluation that considers both adaptation and generalization. We employ pixel-to-pixel modelling as a practical and flexible way to switch between pre-training and single-stream evaluation as well as between arbitrary tasks, without ever requiring changes to models and always using the same pixel loss. Equipped with this framework we obtained large single-stream learning gains from pre-training with a novel family of future prediction tasks, found that momentum hurts, and that the pace of weight updates matters. The combination of these insights leads to matching the performance of IID learning with batch size 1, when using the same architecture and without costly replay buffers. An overview of the paper is available online at https://sites.google.com/view/one-stream-video.',\n",
       "  'similarity': 0.4286}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\n",
    "    \"computer vision\",\n",
    "    \"automated discovery of interpretable cognitive programs underlying reward-guided behavior \",\n",
    "]\n",
    "search = DocumentSearch()\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for query in queries:\n",
    "    search_results.append(search.search(query, 5))\n",
    "\n",
    "recommender = Recommender()\n",
    "results = recommender.find_similar(ref_idx=1, n=5)\n",
    "\n",
    "#search_results[0]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
