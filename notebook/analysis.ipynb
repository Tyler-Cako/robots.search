{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e418410",
   "metadata": {},
   "source": [
    "Let's compare our search engine to simple google results. We will use the same query, with the exception that our google search will have the added keywords \"google deepmind paper\" to get more precise results. We will then compare the similarity scores between the top 10 abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53556117",
   "metadata": {},
   "source": [
    "eg.\n",
    "\n",
    "query: \"computer vision\"\n",
    "\n",
    "-> google: \"computer vision google deepmind paper\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216dba5",
   "metadata": {},
   "source": [
    "Queries: \"computer vision\", \"LLM\", \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DocumentSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f696cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://deepmind.google/research/publications/49871',\n",
       "  'title': 'Veo 2 15 April 2025                   Revisiting Dynamic Evaluation:Online Adaptation for LLMs',\n",
       "  'authors': 'Amal Rannen-Triki, Jörg Bornschein, Alexandre Galashov, Razvan Pascanu, Michalis Titsias, Marcus Hutter, Andras Gyorgy, Yee Whye Teh',\n",
       "  'tags': \"['language', 'models', 'regression', 'predictive', 'optimization']\",\n",
       "  'abstract': 'We revisit dynamic evaluation, the idea of online adapting the parameters of a language model with gradient descent on a given sequence of test tokens. While it is generally known that adapting the parameters at test-time improves the overall predictive performance, we pay particular attention to the speed of adaptation (in terms of sample efficiency) and computational overhead for performing gradient computation and parameter updates.',\n",
       "  'similarity': 0.21514512282623102},\n",
       " {'url': 'http://deepmind.google/research/publications/127036',\n",
       "  'title': 'Veo 2 15 April 2025                   Effective Kernel Fuzzing with Learned White-box Test Mutators',\n",
       "  'authors': 'Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedro Fonseca, Petros Maniatis',\n",
       "  'tags': \"['kernel', 'snowplow', 'coverage', 'program', 'ml']\",\n",
       "  'abstract': 'Kernel fuzzers rely heavily on program mutation to automatically generate new test programs based on existing ones. In particular, program mutation can alter the test’s control and data flow inside the kernel by inserting new system calls, changing the values of call arguments, or performing other program mutations. However, due to the complexity of the kernel code and its user-space interface, finding the effective mutation that can lead to the desired outcome such as increasing the coverage and reaching a target code location is extremely difficult, even with the widespread use of manually-crafted heuristics. This work proposes Snowplow, a kernel fuzzer that uses a learned white-box test mutator to enhance test mutation. The core of Snowplow is an efficient machine learning model that can learn to predict promising mutations given the test program to mutate, its kernel code coverage, and the desired coverage. Snowplow is demonstrated on argument mutations of the kernel tests, and evaluated on recent Linux kernel releases. When fuzzing the kernels for 24 hours, Snowplow shows a significant speedup of discovering new coverage (4.8×∼5.2×) and achieves higher overall coverage (7.0%∼8.6%). In a 7-day fuzzing campaign, Snowplow discovers 86 previously-unknown crashes. Furthermore, the learned mutator is shown to accelerate directed kernel fuzzing by reaching 19 target code locations 8.5× faster and two additional locations that are missed by the state-of-the-art directed kernel fuzzer.',\n",
       "  'similarity': 0.1831245416555909},\n",
       " {'url': 'https://deepmind.google/research/publications/127036',\n",
       "  'title': 'Veo 2 15 April 2025                   Effective Kernel Fuzzing with Learned White-box Test Mutators',\n",
       "  'authors': 'Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedro Fonseca, Petros Maniatis',\n",
       "  'tags': \"['kernel', 'snowplow', 'coverage', 'program', 'ml']\",\n",
       "  'abstract': 'Kernel fuzzers rely heavily on program mutation to automatically generate new test programs based on existing ones. In particular, program mutation can alter the test’s control and data flow inside the kernel by inserting new system calls, changing the values of call arguments, or performing other program mutations. However, due to the complexity of the kernel code and its user-space interface, finding the effective mutation that can lead to the desired outcome such as increasing the coverage and reaching a target code location is extremely difficult, even with the widespread use of manually-crafted heuristics. This work proposes Snowplow, a kernel fuzzer that uses a learned white-box test mutator to enhance test mutation. The core of Snowplow is an efficient machine learning model that can learn to predict promising mutations given the test program to mutate, its kernel code coverage, and the desired coverage. Snowplow is demonstrated on argument mutations of the kernel tests, and evaluated on recent Linux kernel releases. When fuzzing the kernels for 24 hours, Snowplow shows a significant speedup of discovering new coverage (4.8×∼5.2×) and achieves higher overall coverage (7.0%∼8.6%). In a 7-day fuzzing campaign, Snowplow discovers 86 previously-unknown crashes. Furthermore, the learned mutator is shown to accelerate directed kernel fuzzing by reaching 19 target code locations 8.5× faster and two additional locations that are missed by the state-of-the-art directed kernel fuzzer.',\n",
       "  'similarity': 0.1831245416555909},\n",
       " {'url': 'https://deepmind.google/research/publications/124002',\n",
       "  'title': 'Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning',\n",
       "  'authors': 'Michael Xieyang Liu, Savvas Petridis, Vivian Tsai, Alex Fiannaca, olwal , Michael Terry, Carrie Cai',\n",
       "  'tags': \"['users', 'ai', 'user', 'reasoning', 'models']\",\n",
       "  'abstract': 'Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., \"alert if my toddler is getting into mischief\"), with the MLLM analyzing the camera feed and responding within seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users \"stress test\" sensors on potentially unforeseen scenarios. In a user study, participants reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users\\' \"blind spots\" by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we discuss how unique characteristics of MLLMs--such as hallucinations and inconsistent responses--can impact the sensor-creation process. These findings contribute to the design of future intelligent sensing systems that are intuitive and customizable by everyday users.',\n",
       "  'similarity': 0.10645979872112957},\n",
       " {'url': 'http://deepmind.google/research/publications/124002',\n",
       "  'title': 'Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning',\n",
       "  'authors': 'Michael Xieyang Liu, Savvas Petridis, Vivian Tsai, Alex Fiannaca, olwal , Michael Terry, Carrie Cai',\n",
       "  'tags': \"['users', 'ai', 'user', 'reasoning', 'models']\",\n",
       "  'abstract': 'Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., \"alert if my toddler is getting into mischief\"), with the MLLM analyzing the camera feed and responding within seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users \"stress test\" sensors on potentially unforeseen scenarios. In a user study, participants reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users\\' \"blind spots\" by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we discuss how unique characteristics of MLLMs--such as hallucinations and inconsistent responses--can impact the sensor-creation process. These findings contribute to the design of future intelligent sensing systems that are intuitive and customizable by everyday users.',\n",
       "  'similarity': 0.10645979872112957}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = DocumentSearch()\n",
    "\n",
    "search.search(\"Test\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
