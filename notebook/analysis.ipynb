{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e418410",
   "metadata": {},
   "source": [
    "Let's compare our search engine to simple google results. We will use the same query, with the exception that our google search will have the added keywords \"google deepmind paper\" to get more precise results. We will then compare the similarity scores between the top 10 abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53556117",
   "metadata": {},
   "source": [
    "eg.\n",
    "\n",
    "query: \"computer vision\"\n",
    "\n",
    "-> google: \"computer vision google deepmind paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DocumentSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f696cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://deepmind.google/research/publications/83299',\n",
       "  'title': 'Are vision-language models shape or texture biased and can we steer them?',\n",
       "  'authors': 'Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper',\n",
       "  'tags': \"['vision', 'scale', 'image', 'visual', 'models']\",\n",
       "  'abstract': 'Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. As a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. As expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. While this direct impact of language-informed training on a model’s visual perception is intriguing, it raises further questions on our ability to actively steer a model’s output so that its prediction is based on particular visual cues of the user’s choice. Interestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful.',\n",
       "  'similarity': 0.31998329145786486},\n",
       " {'url': 'http://deepmind.google/research/publications/83299',\n",
       "  'title': 'Are vision-language models shape or texture biased and can we steer them?',\n",
       "  'authors': 'Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper',\n",
       "  'tags': \"['vision', 'scale', 'image', 'visual', 'models']\",\n",
       "  'abstract': 'Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. As a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. As expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. While this direct impact of language-informed training on a model’s visual perception is intriguing, it raises further questions on our ability to actively steer a model’s output so that its prediction is based on particular visual cues of the user’s choice. Interestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful.',\n",
       "  'similarity': 0.31998329145786486},\n",
       " {'url': 'https://deepmind.google/research/publications/32698',\n",
       "  'title': 'Gemini and Whisk with Veo 2 15 April 2025                   Towards In-context Scene Understanding',\n",
       "  'authors': 'Ivana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, Olivier Henaff',\n",
       "  'tags': \"['image', 'tasks', 'models', 'vision', 'video']\",\n",
       "  'abstract': 'In-context learning—the ability to configure a model’s behavior with different prompts—has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol—leveraging attention within and across images—which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime.',\n",
       "  'similarity': 0.22610191181019126},\n",
       " {'url': 'https://deepmind.google/research/publications/33708',\n",
       "  'title': 'Improving neural network representations using human similarity judgments',\n",
       "  'authors': 'Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew Lampinen, Simon Kornblith',\n",
       "  'tags': \"['neural', 'image', 'vision', 'representations', 'features']\",\n",
       "  'abstract': 'Deep neural networks have reached human-level performance on many computer vision tasks. However, the objectives used to train these networks enforce only that similar images are embedded at similar locations in the representation space, and do not directly constrain the global structure of the resulting space. Here, we explore the impact of supervising this global structure by linearly aligning it with human similarity judgments. We find that a naive approach leads to large changes in local representational structure that harm downstream performance. Thus, we propose a novel method that aligns the global structure of representations while preserving their local structure. This global-local transform considerably improves accuracy across a variety of few-shot learning and anomaly detection tasks. Our results indicate that human visual representations are globally organized in a way that facilitates learning from few examples, and incorporating this global structure into neural network representations improves performance on downstream tasks.',\n",
       "  'similarity': 0.22067913549490198},\n",
       " {'url': 'https://deepmind.google/research/publications/132991',\n",
       "  'title': 'Scaling Pre-training to One Hundred Billion Data for Vision Language Models',\n",
       "  'authors': 'Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai*  *',\n",
       "  'tags': \"['scale', 'vision', 'image', 'tasks', 'embedding']\",\n",
       "  'abstract': \"We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.\",\n",
       "  'similarity': 0.20292603631321546}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\n",
    "    \"computer vision\",\n",
    "    \"automated discovery of interpretable cognitive programs underlying reward-guided behavior \",\n",
    "]\n",
    "search = DocumentSearch()\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for query in queries:\n",
    "    search_results.append(search.search(query, 5))\n",
    "\n",
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
