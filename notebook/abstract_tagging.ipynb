{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa8b5e3",
   "metadata": {},
   "source": [
    "TLDR:  \n",
    "embed abstracts,  \n",
    "kmeans clustered by abstracts by semantic meaning,  \n",
    "we grab tags from each cluster and assign them to each URL,  \n",
    "we then run k-means again for x amount of runs,  \n",
    "at the end for each url we add up the amount of times it was assigned each tag and take the top 5 tags. \n",
    "\n",
    "next steps for improving tags:  \n",
    "sort out the meaningless tags manually D:  \n",
    "adjust num runs and k cluster numbers  \n",
    "maybe only use the top 75ish most popular tags?  \n",
    "Right now there are 162 unique tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5451036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\search_engine\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>HTML</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Publish_date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>KMeansTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "      <td>A principal goal of computational neuroscience...</td>\n",
       "      <td>6 February 2025</td>\n",
       "      <td>Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...</td>\n",
       "      <td>['data', 'neural', 'networks', 'features', 'fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "      <td>We provide an empirical investigation of the p...</td>\n",
       "      <td>11 February 2025</td>\n",
       "      <td>Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...</td>\n",
       "      <td>['vision', 'scale', 'image', 'dense', 'text']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Delta Variances - Google DeepMind             ...</td>\n",
       "      <td>Generate videos in Gemini and Whisk with Veo 2...</td>\n",
       "      <td>Decision makers may suffer from uncertainty in...</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>Simon Schmitt, John Shawe-Taylor, Hado van Has...</td>\n",
       "      <td>['planning', 'inference', 'energy', 'variation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Effective Kernel Fuzzing with Learned White-bo...</td>\n",
       "      <td>Veo 2 15 April 2025                   Effectiv...</td>\n",
       "      <td>Kernel fuzzers rely heavily on program mutatio...</td>\n",
       "      <td>1 April 2025</td>\n",
       "      <td>Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...</td>\n",
       "      <td>['hash', 'kernel', 'keys', 'snowplow', 'covera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>TIPS: Text-Image Pretraining with Spatial awar...</td>\n",
       "      <td>Whisk with Veo 2 15 April 2025                ...</td>\n",
       "      <td>While image-text representation learning has b...</td>\n",
       "      <td>10 March 2025</td>\n",
       "      <td>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...</td>\n",
       "      <td>['vision', 'scale', 'image', 'dense', 'text']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://deepmind.google/research/publications/...   \n",
       "1  https://deepmind.google/research/publications/...   \n",
       "2  https://deepmind.google/research/publications/...   \n",
       "3  https://deepmind.google/research/publications/...   \n",
       "4  https://deepmind.google/research/publications/...   \n",
       "\n",
       "                                                HTML  \\\n",
       "0  Automated Discovery of Interpretable Cognitive...   \n",
       "1  Scaling Pre-training to One Hundred Billion Da...   \n",
       "2  Delta Variances - Google DeepMind             ...   \n",
       "3  Effective Kernel Fuzzing with Learned White-bo...   \n",
       "4  TIPS: Text-Image Pretraining with Spatial awar...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Automated Discovery of Interpretable Cognitive...   \n",
       "1  Scaling Pre-training to One Hundred Billion Da...   \n",
       "2  Generate videos in Gemini and Whisk with Veo 2...   \n",
       "3  Veo 2 15 April 2025                   Effectiv...   \n",
       "4  Whisk with Veo 2 15 April 2025                ...   \n",
       "\n",
       "                                            Abstract      Publish_date  \\\n",
       "0  A principal goal of computational neuroscience...   6 February 2025   \n",
       "1  We provide an empirical investigation of the p...  11 February 2025   \n",
       "2  Decision makers may suffer from uncertainty in...  20 February 2025   \n",
       "3  Kernel fuzzers rely heavily on program mutatio...      1 April 2025   \n",
       "4  While image-text representation learning has b...     10 March 2025   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...   \n",
       "1  Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...   \n",
       "2  Simon Schmitt, John Shawe-Taylor, Hado van Has...   \n",
       "3  Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...   \n",
       "4  Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...   \n",
       "\n",
       "                                          KMeansTags  \n",
       "0  ['data', 'neural', 'networks', 'features', 'fe...  \n",
       "1      ['vision', 'scale', 'image', 'dense', 'text']  \n",
       "2  ['planning', 'inference', 'energy', 'variation...  \n",
       "3  ['hash', 'kernel', 'keys', 'snowplow', 'covera...  \n",
       "4      ['vision', 'scale', 'image', 'dense', 'text']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('cleaned_url_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f69c9",
   "metadata": {},
   "source": [
    "This creates semantic embeddings of the abstracts, this will allow us to cluster each paper semantically when using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798fc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "abstract_embeddings = model.encode(df['Abstract'])\n",
    "print(abstract_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de4cf2",
   "metadata": {},
   "source": [
    "Manual implementation of kemans using hw 3 as a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b721949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_kmeans(X, k=4, tol=1e-4, max_iter=30):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # random centroids\n",
    "    indices = np.random.choice(n_samples, size=k, replace=False)\n",
    "    centroids = X[indices]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Compute distances and assign clusters\n",
    "        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)  # (n_samples, k)\n",
    "        labels = np.argmin(dists, axis=1)\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)])\n",
    "        \n",
    "        # Check convergence\n",
    "        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n",
    "        if np.all(shifts < tol):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Compute final inertia (sum of squared distances)\n",
    "    final_dists = np.linalg.norm(X - centroids[labels], axis=1)\n",
    "    inertia = np.sum(final_dists ** 2)\n",
    "    \n",
    "    return centroids, labels, inertia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc0d70",
   "metadata": {},
   "source": [
    "Trying to find a optimal k to use, using inertia which is the the sum of the squared distances between each point and its cluster's center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fb962",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'inertia_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m simple_kmeans(abstract_embeddings, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# kmeans.fit(abstract_embeddings)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     inertias\u001b[38;5;241m.\u001b[39mappend(\u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minertia_\u001b[49m)  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Plot the elbow curve\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'inertia_'"
     ]
    }
   ],
   "source": [
    "inertias = []\n",
    "k_values = range(1, 100)  \n",
    "for k in k_values:\n",
    "    centroids, labels, inertia = simple_kmeans(abstract_embeddings, k=k)\n",
    "    # kmeans.fit(abstract_embeddings)\n",
    "    inertias.append(inertia)  \n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Looking for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3bbcb",
   "metadata": {},
   "source": [
    "hmm I want to pick a higher than normal k to get more tags, but there isn't a obvious elbow except maybe at like k = 15ish,   \n",
    "But i want to use a higher k than 15 since I want some more specific tags.  \n",
    "I will try with k = 50 first, meaning theres around 5 papers per cluster  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd93762",
   "metadata": {},
   "source": [
    "Running k means multiple times to get different initial clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kmeans_runs = 20 \n",
    "k = 40\n",
    "\n",
    "labels_all_runs = np.zeros((num_kmeans_runs, len(abstract_embeddings)))  \n",
    "\n",
    "for i in range(num_kmeans_runs):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(abstract_embeddings)\n",
    "    labels_all_runs[i] = kmeans.labels_  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04864d62",
   "metadata": {},
   "source": [
    "Was debating over Tfid or bag of words, bag of words is simple to implement from scratch, but other than that is just a worse algorithm for tagging. Tfid assigns weight to words given all the abstracts allowing it to pick out more signifigant words. It also  uses stopword to prevent only catching the most popular words such as: \"is, and, to, then,\". Learn more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eee87df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'abstract', 'access', 'according', 'accuracy',\n",
       "       'accurate', 'accurately', 'achieve', 'achieved', 'achieves', 'act',\n",
       "       'action', 'actions', 'activations', 'active', 'actually', 'adapt',\n",
       "       'adaptation', 'adapting'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features=1000,\n",
    "                             token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b' )#token pattern is regex for each feature to be as least 2 letters and no numbers\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Abstract'])\n",
    "potential_tags = np.array(vectorizer.get_feature_names_out())\n",
    "potential_tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ab453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "tag_counts_per_url = defaultdict(Counter)\n",
    "\n",
    "# run for each set of clusters created by k-means \n",
    "for labels in labels_all_runs:\n",
    "    cluster_tags = {}\n",
    "\n",
    "    # Extract tags\n",
    "    for cluster_num in range(k):\n",
    "        cluster_indexes = np.where(labels == cluster_num)[0]\n",
    "        if len(cluster_indexes) > 0:\n",
    "            cluster_tfidf = tfidf_matrix[cluster_indexes].mean(axis=0).A1\n",
    "        else:\n",
    "            cluster_tfidf = np.zeros(tfidf_matrix.shape[1])  \n",
    "        top_indices = np.argsort(cluster_tfidf)[::-1][:5]\n",
    "        top_indices = top_indices[::-1][:5]\n",
    "        cluster_tags[cluster_num] = potential_tags[top_indices]\n",
    "\n",
    "    # Assign tags to each URL\n",
    "    for i in range(len(labels)):\n",
    "        for tag in cluster_tags[labels[i]]:\n",
    "            tag_counts_per_url[i][tag] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0c837",
   "metadata": {},
   "source": [
    "Finally we grab the top 5 tags for each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tags = {}\n",
    "for i, counter in tag_counts_per_url.items():\n",
    "    final_tags[i] = [tag for tag, _ in counter.most_common(5)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7261a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique  kmeans tags:  162\n"
     ]
    }
   ],
   "source": [
    "all_final_tags = set(tag for tags in final_tags.values() for tag in tags)\n",
    "print('total unique  kmeans tags: ', len(all_final_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38f823",
   "metadata": {},
   "source": [
    "Now we should probably check the tags against the actual abstracts manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b701c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Automated Discovery of Interpretable Cognitive Programs underlying Reward-guided behavior\n",
      "URL:  https://deepmind.google/research/publications/130468\n",
      "Tags:  ['data', 'neural', 'networks', 'features', 'feature']\n",
      "Title:  Scaling Pre-training to One Hundred Billion Data for Vision Language Models\n",
      "URL:  https://deepmind.google/research/publications/132991\n",
      "Tags:  ['vision', 'scale', 'image', 'dense', 'text']\n",
      "Title:  Generate videos in Gemini and Whisk with Veo 2 15 April 2025                   Delta Variances\n",
      "URL:  https://deepmind.google/research/publications/112791\n",
      "Tags:  ['planning', 'inference', 'energy', 'variational', 'delta']\n",
      "Title:  Veo 2 15 April 2025                   Effective Kernel Fuzzing with Learned White-box Test Mutators\n",
      "URL:  https://deepmind.google/research/publications/127036\n",
      "Tags:  ['hash', 'kernel', 'keys', 'snowplow', 'coverage']\n",
      "Title:  Whisk with Veo 2 15 April 2025                   TIPS: Text-Image Pretraining with Spatial awareness\n",
      "URL:  https://deepmind.google/research/publications/121982\n",
      "Tags:  ['vision', 'scale', 'image', 'dense', 'text']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Title: ', df[\"Title\"][i])\n",
    "    print('URL: ', df[\"URL\"][i])\n",
    "    print('Tags: ', final_tags[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2a7e6",
   "metadata": {},
   "source": [
    "Now did using k-means actually make a difference? Lets check!  \n",
    "Baseline is what TFID would have done by itself on each abstract no k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee14baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique  baseline tags:  542\n"
     ]
    }
   ],
   "source": [
    "baseline_tags = {} #No k-means\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    row = tfidf_matrix[i].toarray().flatten()\n",
    "    top_indices = np.argsort(row)[::-1][:5] \n",
    "    baseline_tags[i] = list(potential_tags[top_indices])\n",
    "\n",
    "all_baseline_tags = set(tag for tags in baseline_tags.values() for tag in tags)\n",
    "print('total unique  baseline tags: ', len(all_baseline_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803064ef",
   "metadata": {},
   "source": [
    "We can see that since our total unque tags for kmeans is much lower, this means that k-means was able to generalize the tags/topics better accross the papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e04ac",
   "metadata": {},
   "source": [
    "There is bit a of similarity but the kmeans definitely signifigantly changes the tags generated from each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "59180a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccards similarity  0.2318\n"
     ]
    }
   ],
   "source": [
    "overlaps = []\n",
    "for i in range(len(df)):\n",
    "    set_kmeans = set(final_tags.get(i, [])) \n",
    "    set_baseline = set(baseline_tags.get(i, [])) \n",
    "\n",
    "    intersection = set_kmeans & set_baseline\n",
    "    union = set_kmeans | set_baseline\n",
    "\n",
    "    if union:  \n",
    "        jaccard = len(intersection) / len(union)\n",
    "        overlaps.append(jaccard)\n",
    "\n",
    "print(f\"Jaccards similarity  {np.mean(overlaps):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621694d",
   "metadata": {},
   "source": [
    "Taking a closer look at the different tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "efc7a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Automated Discovery of Interpretable Cognitive Programs underlying Reward-guided behavior\n",
      "KMeans Tags: ['data', 'neural', 'networks', 'features', 'feature']\n",
      "Baseline Tags: ['programs', 'cogfunsearch', 'discover', 'behavioral', 'program']\n",
      "----------------------------------------\n",
      "Title:  Scaling Pre-training to One Hundred Billion Data for Vision Language Models\n",
      "KMeans Tags: ['vision', 'scale', 'image', 'dense', 'text']\n",
      "Baseline Tags: ['scale', 'web', 'diversity', 'gains', 'examples']\n",
      "----------------------------------------\n",
      "Title:  Generate videos in Gemini and Whisk with Veo 2 15 April 2025                   Delta Variances\n",
      "KMeans Tags: ['planning', 'inference', 'energy', 'variational', 'delta']\n",
      "Baseline Tags: ['delta', 'neural', 'uncertainty', 'networks', 'epistemic']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): \n",
    "    print('Title: ', df[\"Title\"][i])\n",
    "    print(\"KMeans Tags:\", final_tags[i])\n",
    "    print(\"Baseline Tags:\", baseline_tags[i])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb8123",
   "metadata": {},
   "source": [
    "To imrpove tagging: I'm currently seeing some tag words that are a bit meaningless. In what we printed above we see that feature and features were keywords from the first abstract, these tags don't really make sense to me right now. To improve this we can add custom stopwords that prevent our TFID model from considering those words. We would have to manually add those stop words though.  \n",
    "\n",
    "Other steps we can do to improve tagging is messing around with k for k means, maybe we can select random ks between a range(this might be annmoying to implemnt )? k =40 was kind of arbitually selected. Also we can look into howv many iterations of k means to do. \n",
    "\n",
    "Another thing we can do to clean the tags is to count up the numbers of each tag and say we only want the top 75 or so and then remove tags from urls that arent in the top 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d4933bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>HTML</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Publish_date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>KMeansTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "      <td>A principal goal of computational neuroscience...</td>\n",
       "      <td>6 February 2025</td>\n",
       "      <td>Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...</td>\n",
       "      <td>[data, neural, networks, features, feature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "      <td>We provide an empirical investigation of the p...</td>\n",
       "      <td>11 February 2025</td>\n",
       "      <td>Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...</td>\n",
       "      <td>[vision, scale, image, dense, text]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Delta Variances - Google DeepMind             ...</td>\n",
       "      <td>Generate videos in Gemini and Whisk with Veo 2...</td>\n",
       "      <td>Decision makers may suffer from uncertainty in...</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>Simon Schmitt, John Shawe-Taylor, Hado van Has...</td>\n",
       "      <td>[planning, inference, energy, variational, delta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Effective Kernel Fuzzing with Learned White-bo...</td>\n",
       "      <td>Veo 2 15 April 2025                   Effectiv...</td>\n",
       "      <td>Kernel fuzzers rely heavily on program mutatio...</td>\n",
       "      <td>1 April 2025</td>\n",
       "      <td>Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...</td>\n",
       "      <td>[hash, kernel, keys, snowplow, coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>TIPS: Text-Image Pretraining with Spatial awar...</td>\n",
       "      <td>Whisk with Veo 2 15 April 2025                ...</td>\n",
       "      <td>While image-text representation learning has b...</td>\n",
       "      <td>10 March 2025</td>\n",
       "      <td>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...</td>\n",
       "      <td>[vision, scale, image, dense, text]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://deepmind.google/research/publications/...   \n",
       "1  https://deepmind.google/research/publications/...   \n",
       "2  https://deepmind.google/research/publications/...   \n",
       "3  https://deepmind.google/research/publications/...   \n",
       "4  https://deepmind.google/research/publications/...   \n",
       "\n",
       "                                                HTML  \\\n",
       "0  Automated Discovery of Interpretable Cognitive...   \n",
       "1  Scaling Pre-training to One Hundred Billion Da...   \n",
       "2  Delta Variances - Google DeepMind             ...   \n",
       "3  Effective Kernel Fuzzing with Learned White-bo...   \n",
       "4  TIPS: Text-Image Pretraining with Spatial awar...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Automated Discovery of Interpretable Cognitive...   \n",
       "1  Scaling Pre-training to One Hundred Billion Da...   \n",
       "2  Generate videos in Gemini and Whisk with Veo 2...   \n",
       "3  Veo 2 15 April 2025                   Effectiv...   \n",
       "4  Whisk with Veo 2 15 April 2025                ...   \n",
       "\n",
       "                                            Abstract      Publish_date  \\\n",
       "0  A principal goal of computational neuroscience...   6 February 2025   \n",
       "1  We provide an empirical investigation of the p...  11 February 2025   \n",
       "2  Decision makers may suffer from uncertainty in...  20 February 2025   \n",
       "3  Kernel fuzzers rely heavily on program mutatio...      1 April 2025   \n",
       "4  While image-text representation learning has b...     10 March 2025   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...   \n",
       "1  Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...   \n",
       "2  Simon Schmitt, John Shawe-Taylor, Hado van Has...   \n",
       "3  Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...   \n",
       "4  Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...   \n",
       "\n",
       "                                          KMeansTags  \n",
       "0        [data, neural, networks, features, feature]  \n",
       "1                [vision, scale, image, dense, text]  \n",
       "2  [planning, inference, energy, variational, delta]  \n",
       "3           [hash, kernel, keys, snowplow, coverage]  \n",
       "4                [vision, scale, image, dense, text]  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['KMeansTags'] = df.index.map(lambda i: final_tags.get(i, []))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f3f4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_url_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cba118f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feels like cheating but likeee\n",
    "kw_model = KeyBERT('all-MiniLM-L6-v2')\n",
    "\n",
    "keywords = kw_model.extract_keywords(df[\"Abstract\"], top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e690f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags:  ['data', 'neural', 'networks', 'features', 'feature']\n",
      "ketbert:  [('behavioral', 0.408), ('reward', 0.3432), ('neural', 0.3415), ('program', 0.3414), ('predictive', 0.3361)]\n",
      "Tags:  ['vision', 'scale', 'image', 'dense', 'text']\n",
      "ketbert:  [('multilinguality', 0.4022), ('multimodal', 0.3813), ('captions', 0.3723), ('diversity', 0.3378), ('cultural', 0.3288)]\n",
      "Tags:  ['planning', 'inference', 'energy', 'variational', 'delta']\n",
      "ketbert:  [('uncertainty', 0.4599), ('variances', 0.3109), ('networks', 0.2981), ('empirically', 0.2965), ('variance', 0.2913)]\n",
      "Tags:  ['hash', 'kernel', 'keys', 'snowplow', 'coverage']\n",
      "ketbert:  [('fuzzing', 0.4505), ('mutation', 0.4053), ('mutations', 0.3719), ('coverage', 0.35), ('kernels', 0.3409)]\n",
      "Tags:  ['vision', 'scale', 'image', 'dense', 'text']\n",
      "ketbert:  [('captions', 0.3986), ('deepmind', 0.377), ('text', 0.3234), ('supervised', 0.3084), ('learning', 0.2996)]\n",
      "Tags:  ['causal', 'time', 'arrow', 'coarse', 'entropy']\n",
      "ketbert:  [('entropy', 0.5618), ('thermodynamics', 0.4321), ('ensembles', 0.3401), ('markovian', 0.3389), ('probabilistic', 0.3387)]\n",
      "Tags:  ['tasks', 'models', 'reasoning', 'performance', 'language']\n",
      "ketbert:  [('multimodal', 0.4629), ('analogical', 0.4571), ('visual', 0.4524), ('analogies', 0.4405), ('analogy', 0.4362)]\n",
      "Tags:  ['vision', 'scale', 'image', 'dense', 'text']\n",
      "ketbert:  [('perception', 0.4097), ('visual', 0.4061), ('biases', 0.3958), ('bias', 0.3938), ('vision', 0.3786)]\n",
      "Tags:  ['tracr', 'cache', 'ml', 'training', 'coprocessor']\n",
      "ketbert:  [('decoder', 0.3822), ('cache', 0.3708), ('decoding', 0.3609), ('coprocessor', 0.3289), ('embeddings', 0.2875)]\n",
      "Tags:  ['ai', 'generative', 'unlearning', 'targeted', 'people']\n",
      "ketbert:  [('ai', 0.4506), ('creator', 0.4429), ('ghosts', 0.4389), ('generative', 0.4027), ('agent', 0.3818)]\n",
      "Tags:  ['hci', 'agi', 'ai', 'techniques', 'methods']\n",
      "ketbert:  [('ai', 0.5168), ('hci', 0.4984), ('agi', 0.4835), ('interaction', 0.3535), ('interface', 0.3517)]\n",
      "Tags:  ['ait', 'kernel', 'algorithmic', 'ml', 'compression']\n",
      "ketbert:  [('compression', 0.3832), ('kernels', 0.3735), ('kernel', 0.3351), ('sparse', 0.3035), ('complexity', 0.3)]\n",
      "Tags:  ['tasks', 'models', 'reasoning', 'performance', 'language']\n",
      "ketbert:  [('multimodal', 0.4096), ('ai', 0.3961), ('prompting', 0.3401), ('eliciting', 0.3189), ('intelligent', 0.3124)]\n",
      "Tags:  ['par', 'agent', 'pose', 'contact', 'forecasting']\n",
      "ketbert:  [('predicting', 0.4424), ('forecasting', 0.4354), ('prediction', 0.416), ('forecasts', 0.4126), ('agent', 0.407)]\n",
      "Tags:  ['learning', 'rl', 'reinforcement', 'distributional', 'policy']\n",
      "ketbert:  [('ai', 0.4249), ('reinforcement', 0.4121), ('agents', 0.3275), ('hacking', 0.3124), ('oversight', 0.2893)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print('Tags: ', final_tags[i])\n",
    "    print('ketbert: ', keywords[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa0225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
