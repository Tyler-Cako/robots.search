{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99b3878",
   "metadata": {},
   "source": [
    "Read in dataset with keywords and authors.\n",
    "Compute similarity of a given paper to all others via following steps:\n",
    "\n",
    "+ Jaccard sim of keywords\n",
    "+ Jaccard sim of authors\n",
    "+ sum these 2\n",
    "\n",
    "Return top *n* most similar papers to starting paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bcfad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>KMeansTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...</td>\n",
       "      <td>['programs', 'reward', 'rl', 'tasks', 'models']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...</td>\n",
       "      <td>['scale', 'vision', 'image', 'tasks', 'embeddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simon Schmitt, John Shawe-Taylor, Hado van Has...</td>\n",
       "      <td>['neural', 'uncertainty', 'delta', 'weather', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...</td>\n",
       "      <td>['kernel', 'snowplow', 'coverage', 'program', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...</td>\n",
       "      <td>['image', 'tasks', 'vision', 'video', 'scale']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Jason Milionis,  Christos Papadimitriou, Georg...</td>\n",
       "      <td>['game', 'nash', 'games', 'equilibria', 'regret']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>AJ Piergiovanni, Isaac Noble, Dahun Kim, Micha...</td>\n",
       "      <td>['video', 'audio', 'vision', 'point', 'models']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>David Lindner, János Kramár, Sebastian Farquha...</td>\n",
       "      <td>['programs', 'cache', 'ai', 'users', 'reasoning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Meredith Ringel Morris, Jascha Sohl-dickstein,...</td>\n",
       "      <td>['agi', 'ai', 'hci', 'evaluation', 'intelligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Piotr Mirowski, Juliette Love, Kory Mathewson,...</td>\n",
       "      <td>['comedy', 'ai', 'participants', 'dramatron', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Authors  \\\n",
       "0    Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...   \n",
       "1    Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...   \n",
       "2    Simon Schmitt, John Shawe-Taylor, Hado van Has...   \n",
       "3    Sishuai Gong, Wang Rui, Deniz Altinbüken, Pedr...   \n",
       "4    Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...   \n",
       "..                                                 ...   \n",
       "273  Jason Milionis,  Christos Papadimitriou, Georg...   \n",
       "274  AJ Piergiovanni, Isaac Noble, Dahun Kim, Micha...   \n",
       "275  David Lindner, János Kramár, Sebastian Farquha...   \n",
       "276  Meredith Ringel Morris, Jascha Sohl-dickstein,...   \n",
       "277  Piotr Mirowski, Juliette Love, Kory Mathewson,...   \n",
       "\n",
       "                                            KMeansTags  \n",
       "0      ['programs', 'reward', 'rl', 'tasks', 'models']  \n",
       "1    ['scale', 'vision', 'image', 'tasks', 'embeddi...  \n",
       "2    ['neural', 'uncertainty', 'delta', 'weather', ...  \n",
       "3    ['kernel', 'snowplow', 'coverage', 'program', ...  \n",
       "4       ['image', 'tasks', 'vision', 'video', 'scale']  \n",
       "..                                                 ...  \n",
       "273  ['game', 'nash', 'games', 'equilibria', 'regret']  \n",
       "274    ['video', 'audio', 'vision', 'point', 'models']  \n",
       "275  ['programs', 'cache', 'ai', 'users', 'reasoning']  \n",
       "276  ['agi', 'ai', 'hci', 'evaluation', 'intelligen...  \n",
       "277  ['comedy', 'ai', 'participants', 'dramatron', ...  \n",
       "\n",
       "[278 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "original_dataset = pd.read_csv(\"cleaned_url_data.csv\", dtype={\"Authors\": str, \"KMeansTags\": object})\n",
    "dataset = original_dataset[[\"Authors\", \"KMeansTags\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f54a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_auth_list(s: str, delimiter = \",\"):\n",
    "    authors = s.split(delimiter)\n",
    "    authors = [author.strip() for author in authors]\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d635e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [programs, reward, rl, tasks, models]\n",
       "1             [scale, vision, image, tasks, embedding]\n",
       "2      [neural, uncertainty, delta, weather, networks]\n",
       "3            [kernel, snowplow, coverage, program, ml]\n",
       "4                 [image, tasks, vision, video, scale]\n",
       "                            ...                       \n",
       "273            [game, nash, games, equilibria, regret]\n",
       "274              [video, audio, vision, point, models]\n",
       "275            [programs, cache, ai, users, reasoning]\n",
       "276           [agi, ai, hci, evaluation, intelligence]\n",
       "277       [comedy, ai, participants, dramatron, group]\n",
       "Name: KMeansTags, Length: 278, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[:,\"Authors\"] = [str_to_auth_list(authors) for authors in dataset[\"Authors\"]]\n",
    "dataset.loc[:,\"KMeansTags\"] = [str_to_auth_list(tags[2:-2], delimiter=\"', '\") for tags in dataset[\"KMeansTags\"]]\n",
    "dataset[\"KMeansTags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "211465ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_similar_papers(ref_idx = 0, n = 10):\n",
    "    '''\n",
    "    returns list of tuples: (paper_index, similarity score)\n",
    "    '''\n",
    "    paper_count = dataset.shape[0]\n",
    "\n",
    "    assert ref_idx < paper_count\n",
    "    assert n <= paper_count\n",
    "\n",
    "    ref_authors = set(dataset[\"Authors\"][ref_idx])\n",
    "    ref_kw = set(dataset[\"KMeansTags\"][ref_idx])\n",
    "    \n",
    "    similarities = []\n",
    "    for paper in range(paper_count):\n",
    "        authors_set = set(dataset[\"Authors\"][paper])\n",
    "        kw_set = set(dataset[\"KMeansTags\"][paper])\n",
    "        \n",
    "        authors_intersection = authors_set.intersection(ref_authors)\n",
    "        authors_union = authors_set.union(ref_authors)\n",
    "\n",
    "        kw_intersection = kw_set.intersection(ref_kw)\n",
    "        kw_union = kw_set.union(ref_kw)\n",
    "\n",
    "        similarity_score = len(authors_intersection)/len(authors_union) + len(kw_intersection)/len(kw_union)\n",
    "        similarities.append(similarity_score)\n",
    "\n",
    "    similarity_tups = list(zip(range(paper_count), similarities))\n",
    "    similarity_tups = [tup for tup in similarity_tups if tup[1] != 2] # remove identical papers \n",
    "    similarity_tups.sort(key = lambda tup: tup[1], reverse=True)\n",
    "    return similarity_tups[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dcf4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for paper 1 (Scaling Pre-training to One Hundred Billion Data for Vision Language Models):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>HTML</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Publish_date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>KMeansTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>TIPS: Text-Image Pretraining with Spatial awar...</td>\n",
       "      <td>Whisk with Veo 2 15 April 2025                ...</td>\n",
       "      <td>While image-text representation learning has b...</td>\n",
       "      <td>10 March 2025</td>\n",
       "      <td>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...</td>\n",
       "      <td>['image', 'tasks', 'vision', 'video', 'scale']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>http://deepmind.google/research/publications/1...</td>\n",
       "      <td>TIPS: Text-Image Pretraining with Spatial awar...</td>\n",
       "      <td>Whisk with Veo 2 15 April 2025                ...</td>\n",
       "      <td>While image-text representation learning has b...</td>\n",
       "      <td>10 March 2025</td>\n",
       "      <td>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...</td>\n",
       "      <td>['image', 'tasks', 'vision', 'video', 'scale']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Are vision-language models shape or texture bi...</td>\n",
       "      <td>Are vision-language models shape or texture bi...</td>\n",
       "      <td>Unlike traditional vision-only models, vision ...</td>\n",
       "      <td>22 January 2025</td>\n",
       "      <td>Paul Gavrikov, Jovita Lukasik, Steffen Jung, R...</td>\n",
       "      <td>['vision', 'scale', 'image', 'visual', 'models']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Unsupervised Keypoints with Stable Diffusion -...</td>\n",
       "      <td>Whisk with Veo 2 15 April 2025                ...</td>\n",
       "      <td>We present an innovative approach to infer sem...</td>\n",
       "      <td>29 November 2023</td>\n",
       "      <td>Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hos...</td>\n",
       "      <td>['image', 'video', 'tasks', 'vision', 'neural']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Towards In-context Scene Understanding - Googl...</td>\n",
       "      <td>Gemini and Whisk with Veo 2 15 April 2025     ...</td>\n",
       "      <td>In-context learning—the ability to configure a...</td>\n",
       "      <td>10 December 2023</td>\n",
       "      <td>Ivana Balazevic, David Steiner, Nikhil Parthas...</td>\n",
       "      <td>['image', 'tasks', 'models', 'vision', 'video']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>http://deepmind.google/research/publications/8...</td>\n",
       "      <td>Are vision-language models shape or texture bi...</td>\n",
       "      <td>Are vision-language models shape or texture bi...</td>\n",
       "      <td>Unlike traditional vision-only models, vision ...</td>\n",
       "      <td>22 January 2025</td>\n",
       "      <td>Paul Gavrikov, Jovita Lukasik, Steffen Jung, R...</td>\n",
       "      <td>['vision', 'scale', 'image', 'visual', 'models']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>SODA: Bottleneck Diffusion Models for Represen...</td>\n",
       "      <td>Veo 2 15 April 2025                   SODA: Bo...</td>\n",
       "      <td>We introduce SODA, a self-supervised diffusion...</td>\n",
       "      <td>29 November 2023</td>\n",
       "      <td>Drew A. Hudson, Daniel Zoran, Mateusz Malinows...</td>\n",
       "      <td>['image', 'neural', 'scene', 'video', 'tasks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>MingOfficial: A Ming Official Career Dataset a...</td>\n",
       "      <td>A Ming Official Career Dataset and a Historica...</td>\n",
       "      <td>In Chinese studies, understanding the nuanced ...</td>\n",
       "      <td>6 December 2023</td>\n",
       "      <td>You-Jun Chen*, Hsin-Yi Hsieh*, Yu-Tung Lin*, Y...</td>\n",
       "      <td>['embedding', 'retrieval', 'language', 'scale'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Improving neural network representations using...</td>\n",
       "      <td>Improving neural network representations using...</td>\n",
       "      <td>Deep neural networks have reached human-level ...</td>\n",
       "      <td>10 December 2023</td>\n",
       "      <td>Lukas Muttenthaler, Lorenz Linhardt, Jonas Dip...</td>\n",
       "      <td>['neural', 'image', 'vision', 'representations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Self-supervised video pretraining yields stron...</td>\n",
       "      <td>April 2025                   Self-supervised v...</td>\n",
       "      <td>Humans learn powerful representations of objec...</td>\n",
       "      <td>29 February 2024</td>\n",
       "      <td>Nikhil Parthasarathy, Ali Eslami, Joao Carreir...</td>\n",
       "      <td>['video', 'image', 'tasks', 'models', 'scene']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "4    https://deepmind.google/research/publications/...   \n",
       "204  http://deepmind.google/research/publications/1...   \n",
       "7    https://deepmind.google/research/publications/...   \n",
       "115  https://deepmind.google/research/publications/...   \n",
       "121  https://deepmind.google/research/publications/...   \n",
       "199  http://deepmind.google/research/publications/8...   \n",
       "122  https://deepmind.google/research/publications/...   \n",
       "134  https://deepmind.google/research/publications/...   \n",
       "157  https://deepmind.google/research/publications/...   \n",
       "171  https://deepmind.google/research/publications/...   \n",
       "\n",
       "                                                  HTML  \\\n",
       "4    TIPS: Text-Image Pretraining with Spatial awar...   \n",
       "204  TIPS: Text-Image Pretraining with Spatial awar...   \n",
       "7    Are vision-language models shape or texture bi...   \n",
       "115  Unsupervised Keypoints with Stable Diffusion -...   \n",
       "121  Towards In-context Scene Understanding - Googl...   \n",
       "199  Are vision-language models shape or texture bi...   \n",
       "122  SODA: Bottleneck Diffusion Models for Represen...   \n",
       "134  MingOfficial: A Ming Official Career Dataset a...   \n",
       "157  Improving neural network representations using...   \n",
       "171  Self-supervised video pretraining yields stron...   \n",
       "\n",
       "                                                 Title  \\\n",
       "4    Whisk with Veo 2 15 April 2025                ...   \n",
       "204  Whisk with Veo 2 15 April 2025                ...   \n",
       "7    Are vision-language models shape or texture bi...   \n",
       "115  Whisk with Veo 2 15 April 2025                ...   \n",
       "121  Gemini and Whisk with Veo 2 15 April 2025     ...   \n",
       "199  Are vision-language models shape or texture bi...   \n",
       "122  Veo 2 15 April 2025                   SODA: Bo...   \n",
       "134  A Ming Official Career Dataset and a Historica...   \n",
       "157  Improving neural network representations using...   \n",
       "171  April 2025                   Self-supervised v...   \n",
       "\n",
       "                                              Abstract      Publish_date  \\\n",
       "4    While image-text representation learning has b...     10 March 2025   \n",
       "204  While image-text representation learning has b...     10 March 2025   \n",
       "7    Unlike traditional vision-only models, vision ...   22 January 2025   \n",
       "115  We present an innovative approach to infer sem...  29 November 2023   \n",
       "121  In-context learning—the ability to configure a...  10 December 2023   \n",
       "199  Unlike traditional vision-only models, vision ...   22 January 2025   \n",
       "122  We introduce SODA, a self-supervised diffusion...  29 November 2023   \n",
       "134  In Chinese studies, understanding the nuanced ...   6 December 2023   \n",
       "157  Deep neural networks have reached human-level ...  10 December 2023   \n",
       "171  Humans learn powerful representations of objec...  29 February 2024   \n",
       "\n",
       "                                               Authors  \\\n",
       "4    Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...   \n",
       "204  Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Gho...   \n",
       "7    Paul Gavrikov, Jovita Lukasik, Steffen Jung, R...   \n",
       "115  Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hos...   \n",
       "121  Ivana Balazevic, David Steiner, Nikhil Parthas...   \n",
       "199  Paul Gavrikov, Jovita Lukasik, Steffen Jung, R...   \n",
       "122  Drew A. Hudson, Daniel Zoran, Mateusz Malinows...   \n",
       "134  You-Jun Chen*, Hsin-Yi Hsieh*, Yu-Tung Lin*, Y...   \n",
       "157  Lukas Muttenthaler, Lorenz Linhardt, Jonas Dip...   \n",
       "171  Nikhil Parthasarathy, Ali Eslami, Joao Carreir...   \n",
       "\n",
       "                                            KMeansTags  \n",
       "4       ['image', 'tasks', 'vision', 'video', 'scale']  \n",
       "204     ['image', 'tasks', 'vision', 'video', 'scale']  \n",
       "7     ['vision', 'scale', 'image', 'visual', 'models']  \n",
       "115    ['image', 'video', 'tasks', 'vision', 'neural']  \n",
       "121    ['image', 'tasks', 'models', 'vision', 'video']  \n",
       "199   ['vision', 'scale', 'image', 'visual', 'models']  \n",
       "122     ['image', 'neural', 'scene', 'video', 'tasks']  \n",
       "134  ['embedding', 'retrieval', 'language', 'scale'...  \n",
       "157  ['neural', 'image', 'vision', 'representations...  \n",
       "171     ['video', 'image', 'tasks', 'models', 'scene']  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_idx = 1\n",
    "res = n_most_similar_papers(ref_idx)\n",
    "indices = [i for i, _ in res]\n",
    "\n",
    "print(f\"Recommendations for paper {ref_idx} ({original_dataset.loc[ref_idx, \"Title\"]}):\")\n",
    "original_dataset.iloc[indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
