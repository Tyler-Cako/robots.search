{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a8e11a",
   "metadata": {},
   "source": [
    "Read in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb2cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>HTML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://deepmind.google/research/publications/</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0     https://deepmind.google/research/publications/   \n",
       "1  https://deepmind.google/research/publications/...   \n",
       "2  https://deepmind.google/research/publications/...   \n",
       "3  https://deepmind.google/research/publications/...   \n",
       "4  https://deepmind.google/research/publications/...   \n",
       "\n",
       "                                                HTML  \n",
       "0  Publications - Google DeepMind                ...  \n",
       "1  Publications - Google DeepMind                ...  \n",
       "2  Automated Discovery of Interpretable Cognitive...  \n",
       "3  Scaling Pre-training to One Hundred Billion Da...  \n",
       "4  Publications - Google DeepMind                ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('../webscraper/raw_data.csv')\n",
    "\n",
    "df.head()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e221748",
   "metadata": {},
   "source": [
    "Clean the abstacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Pre-training to One Hundred Billion Data for Vision Language Models - Google DeepMind                                      Jump to Content                     Google     DeepMind       Search...                 Search Close                            Google     DeepMind                                      About                                                      Learn about Google DeepMind                                             — Our mission is to build AI responsibly to benefit humanity                             Responsibility & Safety                                             — We want AI to benefit the world, so we must be thoughtful about how it’s built and used                             Education                                             — Our vision is to help make the AI ecosystem more representative of society                             Careers                                             — Many disciplines, one common goal                               Research                                                      View Research                                             — We work on some of the most complex and interesting challenges in AI.                             Breakthroughs                                             — Explore some of the biggest innovations in AI                             Publications                                             — Explore a selection of our recent research                               Technologies                                                      View Technologies                                             — Solving the world’s most complex challenges                             Gemini                                             — The most general and capable AI models we've ever built                             Project Astra                                             — A universal AI agent that is helpful in everyday life                             Imagen                                             — Our highest quality text-to-image model                             Veo                                       — Our state-of-the-art video generation model                             Overview                                                            Veo 2                         (New)                      AlphaFold                                       — Accelerating breakthroughs in biology with AI                             Overview                                                            Impact stories                                                            AlphaFold Server                                                            AlphaFold Database                                             SynthID                                             — Identifying AI-generated content                               Discover                                                      View Discover                                             — Discover our latest breakthroughs and see how we’re shaping the future                             Blog                                             — Discover our latest AI breakthroughs, projects, and updates                             Events                                             — Meet our team and learn more about our research                             The Podcast                                             — Uncover the extraordinary ways AI is transforming our world                       Search...                  Search Close                                   Learn about Google DeepMind                                          Responsibility & Safety                                             — We want AI to benefit the world, so we must be thoughtful about how it’s built and used                            Education                                             — Our vision is to help make the AI ecosystem more representative of society                            Careers                                             — Many disciplines, one common goal                 Latest posts        Introducing Gemini 2.5 Flash 17 April 2025            Generate videos in Gemini and Whisk with Veo 2 15 April 2025                           View Research                                          Breakthroughs                                             — Explore some of the biggest innovations in AI                            Publications                                             — Explore a selection of our recent research                 Latest research posts        DolphinGemma: How Google AI is helping decode dolphin communication 14 April 2025            Google DeepMind at NeurIPS 2024 5 December 2024                        View Technologies                                          Gemini                                             — The most general and capable AI models we've ever built                            Project Astra                                             — A universal AI agent that is helpful in everyday life                            Imagen                                             — Our highest quality text-to-image model                            Veo                                             — Our state-of-the-art video generation model                            AlphaFold                                             — Accelerating breakthroughs in biology with AI                            SynthID                                             — Identifying AI-generated content                 Latest technology posts        Introducing Gemini 2.5 Flash 17 April 2025            Generate videos in Gemini and Whisk with Veo 2 15 April 2025                           View Discover                                          Blog                                             — Discover our latest AI breakthroughs, projects, and updates                            Events                                             — Meet our team and learn more about our research                            The Podcast                                             — Uncover the extraordinary ways AI is transforming our world                 Latest posts        Introducing Gemini 2.5 Flash 17 April 2025            Generate videos in Gemini and Whisk with Veo 2 15 April 2025                   Scaling Pre-training to One Hundred Billion Data for Vision Language Models  Published 11 February 2025    View publication      Download                   Share                                                  Copy link                   ×                          Abstract We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems. Authors Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai*  * External author  Venue arXiv     Footer links    Follow us                                              About         About Google DeepMind   Responsibility & Safety   Research   Technologies   Blog   Careers        Learn more         Gemini   Veo   Imagen   SynthID                           Sign up for updates on our latest innovations                        Email address           Please enter a valid email (e.g., \"name@example.com\")     I accept Google's Terms and Conditions and acknowledge that my information will be used in accordance with Google's Privacy Policy. Sign up                About Google   Google products   Privacy   Terms   Cookies management controls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "abstracts = df['HTML'].tolist()\n",
    "abstracts = [' '.join(abstract.lower().translate(str.maketrans('', '', string.punctuation)).split()) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02df4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# regex to extract from HTML\n",
    "def extract_abstract(text):\n",
    "    match = re.search(r'(?i)abstract\\s*(.*?)(?=\\s*authors)', text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "def extract_publish_date(text):\n",
    "    match = re.search(r'(?i)published\\s+([0-9]{1,2}\\s+\\w+\\s+[0-9]{4})', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "def extract_authors(text):\n",
    "    match = re.search(r'Authors\\s+(.*?)(?=\\s*(External|Venue|Published|Footer|$))', text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "def extract_title(text):\n",
    "    match = re.search(r'([A-Z][^\\n]{10,100}?)\\s+Published', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f3d8a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['HTML'].apply(extract_title)\n",
    "df['cleaned_abstract'] = df['HTML'].apply(extract_abstract)\n",
    "df['publish_date'] = df['HTML'].apply(extract_publish_date)\n",
    "df['authors'] = df['HTML'].apply(extract_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8dbd68b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://deepmind.google/research/publications/132991\n",
      "Scaling Pre-training to One Hundred Billion Data for Vision Language Models\n",
      "Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai*  *\n",
      "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.\n",
      "11 February 2025\n"
     ]
    }
   ],
   "source": [
    "# checking extractions\n",
    "print(df['URL'][3])\n",
    "print(df['title'][3])\n",
    "print(df['authors'][3])\n",
    "print(df['cleaned_abstract'][3])\n",
    "print(df['publish_date'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4e527b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>HTML</th>\n",
       "      <th>cleaned_abstract</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://deepmind.google/research/publications/</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Marcus Hutter</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>A. Feder Cooper, Christopher A. Choquette-Choo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "      <td>A principal goal of computational neuroscience...</td>\n",
       "      <td>6 February 2025</td>\n",
       "      <td>Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...</td>\n",
       "      <td>Automated Discovery of Interpretable Cognitive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "      <td>We provide an empirical investigation of the p...</td>\n",
       "      <td>11 February 2025</td>\n",
       "      <td>Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...</td>\n",
       "      <td>Scaling Pre-training to One Hundred Billion Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepmind.google/research/publications/...</td>\n",
       "      <td>Publications - Google DeepMind                ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Marcus Hutter</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0     https://deepmind.google/research/publications/   \n",
       "1  https://deepmind.google/research/publications/...   \n",
       "2  https://deepmind.google/research/publications/...   \n",
       "3  https://deepmind.google/research/publications/...   \n",
       "4  https://deepmind.google/research/publications/...   \n",
       "\n",
       "                                                HTML  \\\n",
       "0  Publications - Google DeepMind                ...   \n",
       "1  Publications - Google DeepMind                ...   \n",
       "2  Automated Discovery of Interpretable Cognitive...   \n",
       "3  Scaling Pre-training to One Hundred Billion Da...   \n",
       "4  Publications - Google DeepMind                ...   \n",
       "\n",
       "                                    cleaned_abstract      publish_date  \\\n",
       "0                                               None              None   \n",
       "1                                               None              None   \n",
       "2  A principal goal of computational neuroscience...   6 February 2025   \n",
       "3  We provide an empirical investigation of the p...  11 February 2025   \n",
       "4                                               None              None   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                      Marcus Hutter   \n",
       "1  A. Feder Cooper, Christopher A. Choquette-Choo...   \n",
       "2  Pablo Castro Rivadeneira, Kim Stachenfeld, Kev...   \n",
       "3  Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz,...   \n",
       "4                                      Marcus Hutter   \n",
       "\n",
       "                                               title  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  Automated Discovery of Interpretable Cognitive...  \n",
       "3  Scaling Pre-training to One Hundred Billion Da...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2de24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 6)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['cleaned_abstract'])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62087fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_url_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
